{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r11cNiLqvWC6"
      },
      "source": [
        "# üéØ Training a microWakeWord Model\n",
        "\n",
        "<div style=\"background-color: #f0f7fb; padding: 15px; border-radius: 10px; border-left: 5px solid #3498db; margin-bottom: 20px;\">\n",
        "    <h2 style=\"margin-top: 0; color: #3498db;\">Welcome to microWakeWord Training!</h2>\n",
        "    <p>This notebook steps you through training a basic microWakeWord model. It is intended as a <b>starting point</b> for users who want to create their own wake word model. You should use <b>Python 3.10</b> for the best experience.</p>\n",
        "    <p>The training process follows these main steps:</p>\n",
        "    <ol>\n",
        "        <li>Setup the environment and install dependencies</li>\n",
        "        <li>Generate wake word samples using text-to-speech</li>\n",
        "        <li>Download and prepare background audio for training</li>\n",
        "        <li>Set up audio augmentation to create robust training data</li>\n",
        "        <li>Configure and train the neural network model</li>\n",
        "        <li>Export the model for use with ESPHome</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color: #fff3cd; padding: 15px; border-radius: 10px; border-left: 5px solid #f0ad4e; margin-bottom: 20px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #8a6d3b;\">‚ö†Ô∏è Important Note</h3>\n",
        "    <p>The model generated will most likely not be usable for everyday use without experimentation; it may be difficult to trigger or falsely activate too frequently. You will most likely have to experiment with many different settings to obtain a decent model!</p>\n",
        "</div>\n",
        "\n",
        "At the end of this notebook, you will be able to download a tflite file. To use this in ESPHome, you need to write a model manifest JSON file. See the [ESPHome documentation](https://esphome.io/components/micro_wake_word) for the details and the [model repo](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2) for examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjc5l-KG5R6t"
      },
      "source": [
        "## üì¶ Step 1: Setup Environment\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Installs all necessary dependencies for microWakeWord training, including platform-specific requirements.</p>\n",
        "    <p><b>Expected time:</b> 2-5 minutes depending on your internet connection</p>\n",
        "    <p><b>Note:</b> You may need to restart your notebook kernel after this step completes.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BFf6511E65ff",
        "outputId": "eec6c7e8-555a-46a2-9a09-d948c42d8406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/whatsnowplaying/audio-metadata@d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f\n",
            "  Cloning https://github.com/whatsnowplaying/audio-metadata (to revision d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f) to /tmp/pip-req-build-etkz05bu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/whatsnowplaying/audio-metadata /tmp/pip-req-build-etkz05bu\n",
            "  Running command git rev-parse -q --verify 'sha^d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f'\n",
            "  Running command git fetch -q https://github.com/whatsnowplaying/audio-metadata d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f\n",
            "  Running command git checkout -q d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f\n",
            "  Resolved https://github.com/whatsnowplaying/audio-metadata to commit d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=18.2 in /usr/local/lib/python3.11/dist-packages (from audio-metadata==0.12.0) (25.3.0)\n",
            "Collecting bidict==0.* (from audio-metadata==0.12.0)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting bitstruct<9.0,>=6.0 (from audio-metadata==0.12.0)\n",
            "  Downloading bitstruct-8.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting more-itertools<9.0,>=4.0 (from audio-metadata==0.12.0)\n",
            "  Downloading more_itertools-8.14.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pprintpp==0.* (from audio-metadata==0.12.0)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from audio-metadata==0.12.0) (1.17.2)\n",
            "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading bitstruct-8.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading more_itertools-8.14.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: audio-metadata\n",
            "  Building wheel for audio-metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for audio-metadata: filename=audio_metadata-0.12.0-py3-none-any.whl size=53140 sha256=a34115528497bcf5312ba29385bcd2da11069a1b7c715cb69f093f53bb67d314\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/63/dd/4a5b3647219e673100fbb14b6c21033c02a6f0a4203e6c2892\n",
            "Successfully built audio-metadata\n",
            "Installing collected packages: pprintpp, more-itertools, bitstruct, bidict, audio-metadata\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 10.7.0\n",
            "    Uninstalling more-itertools-10.7.0:\n",
            "      Successfully uninstalled more-itertools-10.7.0\n",
            "Successfully installed audio-metadata-0.12.0 bidict-0.23.1 bitstruct-8.21.0 more-itertools-8.14.0 pprintpp-0.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "more_itertools"
                ]
              },
              "id": "cc9de50d51ec4eff8d9f08d3b17a6286"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Cloning into 'microWakeWord'...\n",
            "remote: Enumerating objects: 388, done.\u001b[K\n",
            "remote: Counting objects: 100% (388/388), done.\u001b[K\n",
            "remote: Compressing objects: 100% (224/224), done.\u001b[K\n",
            "remote: Total 388 (delta 215), reused 318 (delta 158), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (388/388), 673.30 KiB | 16.42 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n",
            "Obtaining file:///content/microWakeWord\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting audiomentations (from microwakeword==0.1.0)\n",
            "  Downloading audiomentations-0.42.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: audio_metadata in /usr/local/lib/python3.11/dist-packages (from microwakeword==0.1.0) (0.12.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from microwakeword==0.1.0) (2.14.4)\n",
            "Collecting mmap_ninja (from microwakeword==0.1.0)\n",
            "  Downloading mmap_ninja-0.7.4-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from microwakeword==0.1.0) (2.0.2)\n",
            "Collecting pymicro-features (from microwakeword==0.1.0)\n",
            "  Downloading pymicro_features-1.0.0.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from microwakeword==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: tensorflow>=2.16 in /usr/local/lib/python3.11/dist-packages (from microwakeword==0.1.0) (2.18.0)\n",
            "Collecting webrtcvad (from microwakeword==0.1.0)\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of microwakeword to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Package 'microwakeword' requires a different Python: 3.11.13 not in '<3.11,>=3.10'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installs microWakeWord. Be sure to restart the session after this is finished.\n",
        "import platform\n",
        "\n",
        "if platform.system() == \"Darwin\":\n",
        "    # `pymicro-features` is installed from a fork to support building on macOS\n",
        "    !pip install 'git+https://github.com/puddly/pymicro-features@puddly/minimum-cpp-version'\n",
        "\n",
        "# `audio-metadata` is installed from a fork to unpin `attrs` from a version that breaks Jupyter\n",
        "!pip install 'git+https://github.com/whatsnowplaying/audio-metadata@d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f'\n",
        "\n",
        "# Install ipywidgets for interactive notebook elements\n",
        "!pip install ipywidgets\n",
        "\n",
        "!git clone https://github.com/BigPappy098/microWakeWord\n",
        "!pip install -e ./microWakeWord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYBPNCLx5R6v"
      },
      "source": [
        "## üîä Step 2: Generate Wake Word Samples\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Generates a single sample of your wake word using text-to-speech so you can verify it sounds correct.</p>\n",
        "    <p><b>Key parameter to modify:</b></p>\n",
        "    <ul>\n",
        "        <li><code>target_word</code> - Set this to your desired wake word (use underscores instead of spaces)</li>\n",
        "    </ul>\n",
        "    <p><b>Tips:</b></p>\n",
        "    <ul>\n",
        "        <li>Try phonetic spellings for better pronunciation (e.g., \"hey_komputer\" instead of \"hey_computer\")</li>\n",
        "        <li>Listen to the generated sample to verify it sounds correct before proceeding</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dEluu7nL7ywd",
        "outputId": "2dce119e-be5d-4e72-e513-d8d06823bbbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'piper-sample-generator'...\n",
            "remote: Enumerating objects: 142, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 142 (delta 51), reused 49 (delta 43), pack-reused 69 (from 1)\u001b[K\n",
            "Receiving objects: 100% (142/142), 1.03 MiB | 18.89 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n",
            "--2025-09-06 07:22:21--  https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/642029941/73f4af3c-7cf8-4547-a7b9-3bd29e7f3c33?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-06T08%3A09%3A16Z&rscd=attachment%3B+filename%3Den_US-libritts_r-medium.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-06T07%3A09%3A05Z&ske=2025-09-06T08%3A09%3A16Z&sks=b&skv=2018-11-09&sig=B5uW52zLow3OyGEyVtLq8MG0rpnobqoWuXVAUbYbVQY%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NzE0MzY0MiwibmJmIjoxNzU3MTQzMzQyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.sKhIa9DmmCbegFsAwGul-G94sMwIR5_slNAHGqBZeU4&response-content-disposition=attachment%3B%20filename%3Den_US-libritts_r-medium.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-09-06 07:22:22--  https://release-assets.githubusercontent.com/github-production-release-asset/642029941/73f4af3c-7cf8-4547-a7b9-3bd29e7f3c33?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-06T08%3A09%3A16Z&rscd=attachment%3B+filename%3Den_US-libritts_r-medium.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-06T07%3A09%3A05Z&ske=2025-09-06T08%3A09%3A16Z&sks=b&skv=2018-11-09&sig=B5uW52zLow3OyGEyVtLq8MG0rpnobqoWuXVAUbYbVQY%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NzE0MzY0MiwibmJmIjoxNzU3MTQzMzQyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.sKhIa9DmmCbegFsAwGul-G94sMwIR5_slNAHGqBZeU4&response-content-disposition=attachment%3B%20filename%3Den_US-libritts_r-medium.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 204089915 (195M) [application/octet-stream]\n",
            "Saving to: ‚Äòpiper-sample-generator/models/en_US-libritts_r-medium.pt‚Äô\n",
            "\n",
            "piper-sample-genera 100%[===================>] 194.63M   109MB/s    in 1.8s    \n",
            "\n",
            "2025-09-06 07:22:24 (109 MB/s) - ‚Äòpiper-sample-generator/models/en_US-libritts_r-medium.pt‚Äô saved [204089915/204089915]\n",
            "\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting piper-phonemize-cross==1.2.1\n",
            "  Downloading piper_phonemize_cross-1.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (325 bytes)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pip-TTS (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pip-TTS\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, huggingface_hub, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.33.4\n",
            "    Uninstalling huggingface-hub-0.33.4:\n",
            "      Successfully uninstalled huggingface-hub-0.33.4\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0 huggingface_hub-0.34.4\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/piper-sample-generator/generate_samples.py\", line 15, in <module>\n",
            "    from piper import PiperVoice, SynthesisConfig\n",
            "ModuleNotFoundError: No module named 'piper'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "rate must be specified when data is a numpy array or list of audio samples.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2381234037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 piper-sample-generator/generate_samples.py \"{target_word}\"  --max-samples 1  --batch-size 1  --output-dir generated_samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generated_samples/0.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: rate must be specified when data is a numpy array or list of audio samples."
          ]
        }
      ],
      "source": [
        "# Generates 1 sample of the target word for manual verification.\n",
        "\n",
        "target_word = 'khum_puter'  # Phonetic spellings may produce better samples\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "if not os.path.exists(\"./piper-sample-generator\"):\n",
        "    if platform.system() == \"Darwin\":\n",
        "        !git clone -b mps-support https://github.com/kahrendt/piper-sample-generator\n",
        "    else:\n",
        "        !git clone https://github.com/rhasspy/piper-sample-generator\n",
        "\n",
        "    !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
        "\n",
        "    # Install system dependencies\n",
        "    !pip install torch torchaudio piper-phonemize-cross==1.2.1 piper-TTS\n",
        "    !pip install -U datasets huggingface_hub fsspec\n",
        "\n",
        "    if \"piper-sample-generator/\" not in sys.path:\n",
        "        sys.path.append(\"piper-sample-generator/\")\n",
        "\n",
        "!python3 piper-sample-generator/generate_samples.py \"{target_word}\" \\\n",
        "--max-samples 1 \\\n",
        "--batch-size 1 \\\n",
        "--output-dir generated_samples\n",
        "\n",
        "Audio(\"generated_samples/0.wav\", autoplay=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFrQlw9q5R6v"
      },
      "source": [
        "### üîä Step 2.1: Generate Multiple Wake Word Samples\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Generates a larger set of wake word samples (1000 by default) for training.</p>\n",
        "    <p><b>Key parameters to modify:</b></p>\n",
        "    <ul>\n",
        "        <li><code>--max-samples</code> - Number of samples to generate (default: 1000)</li>\n",
        "        <li><code>--batch-size</code> - How many samples to generate at once (default: 100)</li>\n",
        "    </ul>\n",
        "    <p><b>Advanced options:</b> See the <a href=\"https://github.com/rhasspy/piper-sample-generator\">piper-sample-generator documentation</a> for additional parameters like:</p>\n",
        "    <ul>\n",
        "        <li><code>--noise-scale</code> - Controls voice variation (higher = more variation)</li>\n",
        "        <li><code>--noise-w</code> - Controls speaking style variation</li>\n",
        "        <li><code>--length-scale</code> - Controls speaking speed (higher = slower)</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SvGtCCM9akR"
      },
      "outputs": [],
      "source": [
        "# Generates a larger amount of wake word samples.\n",
        "# Start here when trying to improve your model.\n",
        "# See https://github.com/rhasspy/piper-sample-generator for the full set of\n",
        "# parameters. In particular, experiment with noise-scales and noise-scale-ws,\n",
        "# generating negative samples similar to the wake word, and generating many more\n",
        "# wake word samples, possibly with different phonetic pronunciations.\n",
        "\n",
        "!python3 piper-sample-generator/generate_samples.py \"{target_word}\" \\\n",
        "--max-samples 1000 \\\n",
        "--batch-size 100 \\\n",
        "--output-dir generated_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_FWSjiB5R6v"
      },
      "source": [
        "## üéµ Step 3: Download Background Audio Data\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Downloads audio data for augmentation, including room impulse responses and background noise.</p>\n",
        "    <p><b>Expected time:</b> 10-20 minutes (this step can be slow!)</p>\n",
        "    <p><b>Why this matters:</b> Good background audio is essential for training a robust wake word model that works in real environments.</p>\n",
        "    <p><b>Note:</b> The data downloaded has mixed licenses and should be considered for <b>non-commercial personal use only</b>.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJRG4Qvo9nXG"
      },
      "outputs": [],
      "source": [
        "# Downloads audio data for augmentation. This can be slow!\n",
        "# Borrowed from openWakeWord's automatic_model_training.ipynb, accessed March 4, 2024\n",
        "#\n",
        "# **Important note!** The data downloaded here has a mixture of difference\n",
        "# licenses and usage restrictions. As such, any custom models trained with this\n",
        "# data should be considered as appropriate for **non-commercial** personal use only.\n",
        "\n",
        "\n",
        "import datasets\n",
        "import scipy\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "## Download MIR RIR data\n",
        "\n",
        "output_dir = \"./mit_rirs\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    for row in tqdm(rir_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1]\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "## Download noise and background audio\n",
        "\n",
        "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
        "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
        "# For full-scale training, it's recommended to download the entire dataset from\n",
        "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
        "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
        "\n",
        "if not os.path.exists(\"audioset\"):\n",
        "    os.mkdir(\"audioset\")\n",
        "\n",
        "    fname = \"bal_train09.tar\"\n",
        "    out_dir = f\"audioset/{fname}\"\n",
        "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
        "    !wget -O {out_dir} {link}\n",
        "    !cd audioset && tar -xf bal_train09.tar\n",
        "\n",
        "    output_dir = \"./audioset_16k\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
        "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
        "    for row in tqdm(audioset_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "# Free Music Archive dataset\n",
        "# https://github.com/mdeff/fma\n",
        "# (Third-party mchl914 extra small set)\n",
        "\n",
        "output_dir = \"./fma\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    fname = \"fma_xs.zip\"\n",
        "    link = \"https://huggingface.co/datasets/mchl914/fma_xsmall/resolve/main/\" + fname\n",
        "    out_dir = f\"fma/{fname}\"\n",
        "    !wget -O {out_dir} {link}\n",
        "    !cd {output_dir} && unzip -q {fname}\n",
        "\n",
        "    output_dir = \"./fma_16k\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    fma_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"fma/fma_small\").glob(\"**/*.mp3\")]})\n",
        "    fma_dataset = fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
        "    for row in tqdm(fma_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "168ScZnP5R6w"
      },
      "source": [
        "## üîÑ Step 4: Set Up Audio Augmentation\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Configures audio augmentation to create more varied training samples.</p>\n",
        "    <p><b>Why this matters:</b> Augmentation helps the model learn to recognize your wake word in different environments and conditions.</p>\n",
        "    <p><b>Key parameters to experiment with:</b></p>\n",
        "    <ul>\n",
        "        <li><code>augmentation_probabilities</code> - Chances of applying different audio effects</li>\n",
        "        <li><code>background_min_snr_db</code> and <code>background_max_snr_db</code> - Signal-to-noise ratio range</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW3bmbI5-JAz"
      },
      "outputs": [],
      "source": [
        "# Sets up the augmentations.\n",
        "# To improve your model, experiment with these settings and use more sources of\n",
        "# background clips.\n",
        "\n",
        "from microwakeword.audio.augmentation import Augmentation\n",
        "from microwakeword.audio.clips import Clips\n",
        "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
        "\n",
        "clips = Clips(input_directory='generated_samples',\n",
        "              file_pattern='*.wav',\n",
        "              max_clip_duration_s=None,\n",
        "              remove_silence=False,\n",
        "              random_split_seed=10,\n",
        "              split_count=0.1,\n",
        "              )\n",
        "augmenter = Augmentation(augmentation_duration_s=3.2,\n",
        "                         augmentation_probabilities = {\n",
        "                                \"SevenBandParametricEQ\": 0.1,\n",
        "                                \"TanhDistortion\": 0.1,\n",
        "                                \"PitchShift\": 0.1,\n",
        "                                \"BandStopFilter\": 0.1,\n",
        "                                \"AddColorNoise\": 0.1,\n",
        "                                \"AddBackgroundNoise\": 0.75,\n",
        "                                \"Gain\": 1.0,\n",
        "                                \"RIR\": 0.5,\n",
        "                            },\n",
        "                         impulse_paths = ['mit_rirs'],\n",
        "                         background_paths = ['fma_16k', 'audioset_16k'],\n",
        "                         background_min_snr_db = -5,\n",
        "                         background_max_snr_db = 10,\n",
        "                         min_jitter_s = 0.195,\n",
        "                         max_jitter_s = 0.205,\n",
        "                         )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqJknQ9g5R6w"
      },
      "source": [
        "### üîÑ Step 4.1: Test Audio Augmentation\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Augments a random clip and plays it back so you can verify the augmentation sounds reasonable.</p>\n",
        "    <p><b>What to listen for:</b> The wake word should still be recognizable despite background noise and effects.</p>\n",
        "    <p><b>Tip:</b> If the augmentation is too strong (wake word not audible) or too weak (no background noise), adjust the parameters in the previous cell.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5UsJfKKD1k9"
      },
      "outputs": [],
      "source": [
        "# Augment a random clip and play it back to verify it works well\n",
        "\n",
        "from IPython.display import Audio\n",
        "from microwakeword.audio.audio_utils import save_clip\n",
        "\n",
        "random_clip = clips.get_random_clip()\n",
        "augmented_clip = augmenter.augment_clip(random_clip)\n",
        "save_clip(augmented_clip, 'augmented_clip.wav')\n",
        "\n",
        "Audio(\"augmented_clip.wav\", autoplay=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deXsEE1u5R6w"
      },
      "source": [
        "## üîÑ Step 5: Generate Augmented Features\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Augments samples and saves training, validation, and testing sets.</p>\n",
        "    <p><b>Why this matters:</b> This creates the actual data that will be used to train the neural network.</p>\n",
        "    <p><b>Note:</b> The training set uses more repetition to help the model learn, while the testing set uses a streaming approach to better simulate real-world usage.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7BHcY1mEGbK"
      },
      "outputs": [],
      "source": [
        "# Augment samples and save the training, validation, and testing sets.\n",
        "# Validating and testing samples generated the same way can make the model\n",
        "# benchmark better than it performs in real-word use. Use real samples or TTS\n",
        "# samples generated with a different TTS engine to potentially get more accurate\n",
        "# benchmarks.\n",
        "\n",
        "import os\n",
        "from mmap_ninja.ragged import RaggedMmap\n",
        "\n",
        "output_dir = 'generated_augmented_features'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "splits = [\"training\", \"validation\", \"testing\"]\n",
        "for split in splits:\n",
        "  out_dir = os.path.join(output_dir, split)\n",
        "  if not os.path.exists(out_dir):\n",
        "      os.mkdir(out_dir)\n",
        "\n",
        "\n",
        "  split_name = \"train\"\n",
        "  repetition = 2\n",
        "\n",
        "  spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=10,    # Uses the same spectrogram repeatedly, just shifted over by one frame. This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "  if split == \"validation\":\n",
        "    split_name = \"validation\"\n",
        "    repetition = 1\n",
        "  elif split == \"testing\":\n",
        "    split_name = \"test\"\n",
        "    repetition = 1\n",
        "    spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=1,    # The testing set uses the streaming version of the model, so no artificial repetition is necessary\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "\n",
        "  RaggedMmap.from_generator(\n",
        "      out_dir=os.path.join(out_dir, 'wakeword_mmap'),\n",
        "      sample_generator=spectrograms.spectrogram_generator(split=split_name, repeat=repetition),\n",
        "      batch_size=100,\n",
        "      verbose=True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhlZG7ta5R6x"
      },
      "source": [
        "## üì• Step 6: Download Negative Datasets\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Downloads pre-generated spectrogram features for various negative datasets.</p>\n",
        "    <p><b>Why this matters:</b> Negative samples help the model learn what is NOT your wake word, reducing false activations.</p>\n",
        "    <p><b>Datasets included:</b></p>\n",
        "    <ul>\n",
        "        <li><code>dinner_party</code> - Conversations in a dinner party setting</li>\n",
        "        <li><code>dinner_party_eval</code> - Separate evaluation set of dinner party audio</li>\n",
        "        <li><code>no_speech</code> - Environmental sounds without speech</li>\n",
        "        <li><code>speech</code> - Various speech samples</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pGuJDPyp3ax"
      },
      "outputs": [],
      "source": [
        "# Downloads pre-generated spectrogram features (made for microWakeWord in\n",
        "# particular) for various negative datasets. This can be slow!\n",
        "\n",
        "output_dir = './negative_datasets'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
        "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
        "    for fname in filenames:\n",
        "        link = link_root + fname\n",
        "\n",
        "        zip_path = f\"negative_datasets/{fname}\"\n",
        "        !wget -O {zip_path} {link}\n",
        "        !unzip -q {zip_path} -d {output_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvlmesaf5R6x"
      },
      "source": [
        "## ‚öôÔ∏è Step 7: Configure Training Parameters\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Creates a YAML configuration file that controls the training process.</p>\n",
        "    <p><b>Why this matters:</b> These hyperparameters can make a huge difference in model quality.</p>\n",
        "    <p><b>Key parameters to experiment with:</b></p>\n",
        "    <ul>\n",
        "        <li><code>sampling_weight</code> - Controls how often samples from each dataset are used in training</li>\n",
        "        <li><code>penalty_weight</code> - Controls how much incorrect predictions from each dataset are penalized</li>\n",
        "        <li><code>training_steps</code> - Number of training iterations (increase for potentially better models)</li>\n",
        "        <li><code>positive_class_weight</code> and <code>negative_class_weight</code> - Balance between false positives and false negatives</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii1A14GsGVQT"
      },
      "outputs": [],
      "source": [
        "# Save a yaml config that controls the training process\n",
        "# These hyperparamters can make a huge different in model quality.\n",
        "# Experiment with sampling and penalty weights and increasing the number of\n",
        "# training steps.\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "config = {}\n",
        "\n",
        "config[\"window_step_ms\"] = 10\n",
        "\n",
        "config[\"train_dir\"] = (\n",
        "    \"trained_models/wakeword\"\n",
        ")\n",
        "\n",
        "\n",
        "# Each feature_dir should have at least one of the following folders with this structure:\n",
        "#  training/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#\n",
        "#  sampling_weight: Weight for choosing a spectrogram from this set in the batch\n",
        "#  penalty_weight: Penalizing weight for incorrect predictions from this set\n",
        "#  truth: Boolean whether this set has positive samples or negative samples\n",
        "#  truncation_strategy = If spectrograms in the set are longer than necessary for training, how are they truncated\n",
        "#       - random: choose a random portion of the entire spectrogram - useful for long negative samples\n",
        "#       - truncate_start: remove the start of the spectrogram\n",
        "#       - truncate_end: remove the end of the spectrogram\n",
        "#       - split: Split the longer spectrogram into separate spectrograms offset by 100 ms. Only for ambient sets\n",
        "\n",
        "config[\"features\"] = [\n",
        "    {\n",
        "        \"features_dir\": \"generated_augmented_features\",\n",
        "        \"sampling_weight\": 2.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": True,\n",
        "        \"truncation_strategy\": \"truncate_start\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/speech\",\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/dinner_party\",\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/no_speech\",\n",
        "        \"sampling_weight\": 5.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    { # Only used for validation and testing\n",
        "        \"features_dir\": \"negative_datasets/dinner_party_eval\",\n",
        "        \"sampling_weight\": 0.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"split\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Number of training steps in each iteration - various other settings are configured as lists that corresponds to different steps\n",
        "config[\"training_steps\"] = [10000]\n",
        "\n",
        "# Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
        "config[\"positive_class_weight\"] = [1]\n",
        "config[\"negative_class_weight\"] = [20]\n",
        "\n",
        "config[\"learning_rates\"] = [\n",
        "    0.001,\n",
        "]  # Learning rates for Adam optimizer - list that corresponds to training steps\n",
        "config[\"batch_size\"] = 128\n",
        "\n",
        "config[\"time_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"time_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "\n",
        "config[\"eval_step_interval\"] = (\n",
        "    500  # Test the validation sets after every this many steps\n",
        ")\n",
        "config[\"clip_duration_ms\"] = (\n",
        "    1500  # Maximum length of wake word that the streaming model will accept\n",
        ")\n",
        "\n",
        "# The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
        "# Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
        "# Available metrics:\n",
        "#   - \"loss\" - cross entropy error on validation set\n",
        "#   - \"accuracy\" - accuracy of validation set\n",
        "#   - \"recall\" - recall of validation set\n",
        "#   - \"precision\" - precision of validation set\n",
        "#   - \"false_positive_rate\" - false positive rate of validation set\n",
        "#   - \"false_negative_rate\" - false negative rate of validation set\n",
        "#   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
        "#   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
        "config[\"target_minimization\"] = 0.9\n",
        "config[\"minimization_metric\"] = None  # Set to None to disable\n",
        "\n",
        "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
        "\n",
        "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
        "    documents = yaml.dump(config, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwsMeXNq5R6x"
      },
      "source": [
        "## üöÄ Step 8: Train the Model\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Trains the neural network model using the data and configuration from previous steps.</p>\n",
        "    <p><b>Expected time:</b> 30+ minutes (much faster with a GPU)</p>\n",
        "    <p><b>What to expect:</b> The training process will print progress updates. When finished, it will convert the model to a streaming version suitable for on-device detection.</p>\n",
        "    <p><b>Key parameters to modify:</b></p>\n",
        "    <ul>\n",
        "        <li><code>--train 1</code> - Set to 0 to only convert and test the best-weighted model without training</li>\n",
        "        <li>Neural network architecture parameters at the end of the command</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoEXJBaiC9mf"
      },
      "outputs": [],
      "source": [
        "# Trains a model. When finished, it will quantize and convert the model to a\n",
        "# streaming version suitable for on-device detection.\n",
        "# It will resume if stopped, but it will start over at the configured training\n",
        "# steps in the yaml file.\n",
        "# Change --train 0 to only convert and test the best-weighted model.\n",
        "# On Google colab, it doesn't print the mini-batch results, so it may appear\n",
        "# stuck for several minutes! Additionally, it is very slow compared to training\n",
        "# on a local GPU.\n",
        "\n",
        "!python -m microwakeword.model_train_eval \\\n",
        "--training_config='training_parameters.yaml' \\\n",
        "--train 1 \\\n",
        "--restore_checkpoint 1 \\\n",
        "--test_tf_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming_quantized 0 \\\n",
        "--test_tflite_streaming 0 \\\n",
        "--test_tflite_streaming_quantized 1 \\\n",
        "--use_weights \"best_weights\" \\\n",
        "mixednet \\\n",
        "--pointwise_filters \"64,64,64,64\" \\\n",
        "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
        "--mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
        "--residual_connection \"0,0,0,0\" \\\n",
        "--first_conv_filters 32 \\\n",
        "--first_conv_kernel_size 5 \\\n",
        "--stride 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9buX_qL5R6y"
      },
      "source": [
        "## üì§ Step 9: Export the Model\n",
        "\n",
        "<div style=\"background-color: #e8f4f8; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
        "    <p><b>What this step does:</b> Downloads the trained TFLite model file for use with ESPHome.</p>\n",
        "    <p><b>Next steps:</b></p>\n",
        "    <ol>\n",
        "        <li>Create a model manifest JSON file based on the training results</li>\n",
        "        <li>Adjust the probability threshold based on test results</li>\n",
        "        <li>Upload both files to your ESPHome device</li>\n",
        "    </ol>\n",
        "    <p><b>Resources:</b></p>\n",
        "    <ul>\n",
        "        <li><a href=\"https://esphome.io/components/micro_wake_word\">ESPHome documentation</a></li>\n",
        "        <li><a href=\"https://github.com/esphome/micro-wake-word-models/tree/main/models/v2\">Example model configurations</a></li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex_UIWvwtjAN"
      },
      "outputs": [],
      "source": [
        "# Downloads the tflite model file. To use on the device, you need to write a\n",
        "# Model JSON file. See https://esphome.io/components/micro_wake_word for the\n",
        "# documentation and\n",
        "# https://github.com/esphome/micro-wake-word-models/tree/main/models/v2 for\n",
        "# examples. Adjust the probability threshold based on the test results obtained\n",
        "# after training is finished. You may also need to increase the Tensor arena\n",
        "# model size if the model fails to load.\n",
        "\n",
        "import os\n",
        "\n",
        "# Get the model file path\n",
        "model_path = \"trained_models/wakeword/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\"\n",
        "\n",
        "# Check if running in a Jupyter environment\n",
        "try:\n",
        "    from google.colab import files\n",
        "    # If in Colab, use files.download\n",
        "    files.download(model_path)\n",
        "    print(f\"Model downloaded from {model_path}\")\n",
        "except ImportError:\n",
        "    # If not in Colab, just print the path\n",
        "    print(f\"\\nModel saved at: {os.path.abspath(model_path)}\")\n",
        "    print(\"\\nTo use this model with ESPHome:\")\n",
        "    print(\"1. Create a model manifest JSON file\")\n",
        "    print(\"2. Copy both files to your ESPHome configuration directory\")\n",
        "    print(\"3. Configure ESPHome to use the model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNWJrUHO5R6y"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "<div style=\"background-color: #dff0d8; padding: 15px; border-radius: 10px; border-left: 5px solid #3c763d; margin-bottom: 20px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #3c763d;\">You've Successfully Trained a Wake Word Model!</h3>\n",
        "    <p>You've completed all the steps to train a custom wake word model with microWakeWord. Here's what you can do next:</p>\n",
        "    <ol>\n",
        "        <li><b>Test your model</b> - Try different probability thresholds to balance between detection rate and false positives</li>\n",
        "        <li><b>Experiment</b> - Try different training parameters to improve your model</li>\n",
        "        <li><b>Deploy to ESPHome</b> - Use your model on an ESP32 device</li>\n",
        "    </ol>\n",
        "    <p>Remember that wake word model training is an iterative process. You may need to adjust parameters and retrain several times to get the best results for your specific use case.</p>\n",
        "</div>\n",
        "\n",
        "### Example ESPHome Configuration\n",
        "\n",
        "```yaml\n",
        "# Wake word configuration\n",
        "micro_wake_word:\n",
        "  model_file: \"stream_state_internal_quant.tflite\"\n",
        "  model_name: \"my_wake_word\"\n",
        "  probability_cutoff: 0.5  # Adjust based on training results\n",
        "  \n",
        "binary_sensor:\n",
        "  - platform: micro_wake_word\n",
        "    name: \"Wake Word Detected\"\n",
        "    id: wake_word\n",
        "    model_id: my_wake_word\n",
        "    \n",
        "# Optional - add a text-to-speech response\n",
        "esphome:\n",
        "  on_boot:\n",
        "    priority: -100\n",
        "    then:\n",
        "      - delay: 5s\n",
        "      - logger.log: \"Wake word detection ready\"\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}